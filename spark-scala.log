mnf30:sample_data$ spark-shell
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.0
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc (master = yarn-client, app id = application_1512129765778_0148).
17/12/01 15:52:41 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.1.0-cdh5.12.1
17/12/01 15:52:41 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
SQL context available as sqlContext.

scala> var xml = sc.textFile("Posts.xml")
xml: org.apache.spark.rdd.RDD[String] = Posts.xml MapPartitionsRDD[1] at textFile at <console>:27

scala> xml(1)
<console>:30: error: org.apache.spark.rdd.RDD[String] does not take parameters
              xml(1)
                 ^

scala> xml.length
<console>:30: error: value length is not a member of org.apache.spark.rdd.RDD[String]
              xml.length
                  ^

scala> xml.count()
res2: Long = 201

scala> :paste
// Entering paste mode (ctrl-D to finish)

class Post(val toBeParsed: String) {
    private var postMap = if(isHeaderOrFooter()) null else transformIntoMap()

    private def transformIntoMap() : Map[String, String] = {
        var preParsedRow = toBeParsed.split('"').map(_.replace("<row","")).map(_.replace("/>","")).map(_.trim).filterNot(_.isEmpty)
        var postHeader = preParsedRow.filter(_.endsWith("=")).map(_.replace("=", ""))
        var postContent = preParsedRow.filterNot(_.endsWith("="))
        return (postHeader zip postContent).toMap
    }

    private def isHeaderOrFooter() : Boolean = {
        return (toBeParsed.contains("<?xml version=\"1.0\" encoding=\"utf-8\"?>") || toBeParsed.endsWith("posts>"))
    }

    def getPost() :Map[String,String] = {
        return postMap
    }

    def getId() : Int = {
        return postMap.get("Id").getOrElse("-1").toInt
    }

    def getBody() : String = {
        return postMap.get("Body").getOrElse(null)
    }
}

// Exiting paste mode, now interpreting.

defined class Post

scala> var posts = xml.map(elem => new Posts(elem)).filterNot(_.getPost() == null)
<console>:29: error: not found: type Posts
         var posts = xml.map(elem => new Posts(elem)).filterNot(_.getPost() == null)
                                         ^

scala> var posts = xml.map(elem => new Post(elem)).filterNot(_.getPost() == null)
<console>:30: error: value filterNot is not a member of org.apache.spark.rdd.RDD[Post]
         var posts = xml.map(elem => new Post(elem)).filterNot(_.getPost() == null)
                                                     ^

scala> var posts = xml.map(elem => new Post(elem))
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[2] at map at <console>:30

scala> posts(1)
<console>:33: error: org.apache.spark.rdd.RDD[Post] does not take parameters
              posts(1)
                   ^

scala> posts.take(2)
res4: Array[Post] = Array($iwC$$iwC$Post@50abbf54, $iwC$$iwC$Post@15b59258)

scala> posts.flatMap(_.split(" "))
<console>:33: error: value split is not a member of Post
              posts.flatMap(_.split(" "))
                              ^

scala> posts.flatMap(_.getBody.split(" "))
res6: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at flatMap at <console>:33

scala> res6.take(4)
17/12/01 15:58:46 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3, staff82.elec.qmul.ac.uk, executor 4): java.lang.NullPointerException
	at $line22.$read$$iwC$$iwC$Post.getBody(<console>:36)
	at $line29.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at $line29.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

17/12/01 15:58:46 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 6, staff82.elec.qmul.ac.uk, executor 4): java.lang.NullPointerException
	at $iwC$$iwC$Post.getBody(<console>:36)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1862)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1875)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1888)
	at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.take(RDD.scala:1302)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:42)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:44)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:46)
	at $iwC$$iwC$$iwC.<init>(<console>:48)
	at $iwC$$iwC.<init>(<console>:50)
	at $iwC.<init>(<console>:52)
	at <init>(<console>:54)
	at .<init>(<console>:58)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1045)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1326)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:821)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:800)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1064)
	at org.apache.spark.repl.Main$.main(Main.scala:35)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NullPointerException
	at $iwC$$iwC$Post.getBody(<console>:36)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:33)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1888)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:242)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


scala> res6.get(4)
<console>:35: error: value get is not a member of org.apache.spark.rdd.RDD[String]
              res6.get(4)
                   ^

scala> var posts = xml.map(elem => new Post(elem))
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[4] at map at <console>:30

scala> var posts = xml.map(elem => new Post(elem))
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[5] at map at <console>:30

scala> posts = posts.filterNot(_.getPost == null)
<console>:32: error: value filterNot is not a member of org.apache.spark.rdd.RDD[Post]
         posts = posts.filterNot(_.getPost == null)
                       ^

scala> posts = posts.filter(_.getPost == null)
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[6] at filter at <console>:32

scala> posts = posts.filter(_.getPost != null)
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[7] at filter at <console>:32

scala> var posts = xml.map(elem => new Post(elem))
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[8] at map at <console>:30

scala> posts = posts.filter(_.getPost != null)
posts: org.apache.spark.rdd.RDD[Post] = MapPartitionsRDD[9] at filter at <console>:32

scala> posts.get(1)
<console>:33: error: value get is not a member of org.apache.spark.rdd.RDD[Post]
              posts.get(1)
                    ^

scala> posts._(1)
<console>:1: error: identifier expected but '_' found.
       posts._(1)
             ^
<console>:1: error: ';' expected but '(' found.
       posts._(1)
              ^

scala> posts._1
<console>:33: error: value _1 is not a member of org.apache.spark.rdd.RDD[Post]
              posts._1
                    ^

scala> posts.flatMap(_.getBody.split(" "))
res11: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at flatMap at <console>:33

scala> posts.flatMap(_.getBody.split(" ")).take(2)
res12: Array[String] = Array(&lt;p&gt;I, want)

scala> var wordTuple = posts.flatMap(_.getBody.split(" ")).map(word => (word,1))
wordTuple: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[13] at map at <console>:32

scala> wordTuple.count()
res13: Long = 19985

scala> wordTuple.reduceByKey
<console>:35: error: ambiguous reference to overloaded definition,
both method reduceByKey in class PairRDDFunctions of type (func: (Int, Int) => Int)org.apache.spark.rdd.RDD[(String, Int)]
and  method reduceByKey in class PairRDDFunctions of type (func: (Int, Int) => Int, numPartitions: Int)org.apache.spark.rdd.RDD[(String, Int)]
match expected type ?
              wordTuple.reduceByKey
                        ^

scala> wordTuple.reduceByKey(word,count => (word+count))
<console>:35: error: not found: value word
              wordTuple.reduceByKey(word,count => (word+count))
                                    ^

scala> wordTuple.reduceByKey((word,count) => (word+count))
res16: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[14] at reduceByKey at <console>:35

scala> var reduced = wordTuple.reduceByKey((word,count) => (word+count))
reduced: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at reduceByKey at <console>:34

scala> reduced
res17: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at reduceByKey at <console>:34

scala> reduced.count()
res18: Long = 5604

scala> reduced.take(2)
res19: Array[(String, Int)] = Array((!=,3), (Master.,1))

scala> reduced.take(10)
res20: Array[(String, Int)] = Array((!=,3), (Master.,1), (someone,3), (,fileFormatID,1), (recursion,,1), (simplifications,1), ($id=,1), (components.,1), (date1.DayOfYear&#xA;,2), (order,5))

scala> reduced.take(20).foreach(println)
(!=,3)
(Master.,1)
(someone,3)
(,fileFormatID,1)
(recursion,,1)
(simplifications,1)
($id=,1)
(components.,1)
(date1.DayOfYear&#xA;,2)
(order,5)
(5000.0;&#xA;&lt;/code&gt;&lt;/pre&gt;&#xA;&#xA;&lt;p&gt;Identifying,1)
(type=&quot;text&quot;,2)
((description,,1)
(LINQ,7)
(experience,,1)
(Visual,4)
(behind,1)
((&lt;code&gt;pitimes.c&lt;/code&gt;):&lt;/p&gt;&#xA;&#xA;&lt;pre,1)
(play,,1)
(been,18)

scala>

scala> log(E)
<console>:26: error: not found: value E
              log(E)
                  ^

scala> Math.log(E)
<console>:26: error: not found: value E
              Math.log(E)
                       ^

scala> Math.log()
<console>:26: error: not enough arguments for method log: (x$1: Double)Double.
Unspecified value parameter x$1.
              Math.log()
                      ^

scala> Math.log(1)
res25: Double = 0.0

scala> Math.log(10)
res26: Double = 2.302585092994046

scala> Math.log(Mathe.E)
<console>:26: error: not found: value Mathe
              Math.log(Mathe.E)
                       ^

scala> Math.log(Math.E)
res28: Double = 1.0

scala> Math.E
res29: Double = 2.718281828459045

scala> posts.length
<console>:33: error: value length is not a member of org.apache.spark.rdd.RDD[Post]
              posts.length
                    ^

scala> var totalPosts = posts.count()
totalPosts: Long = 198

scala> totalPosts
res31: Long = 198

scala> var idf = (Math.log(totalPosts) - Math.log(12))/Math.log(Math.E)
idf: Double = 2.803360380906535

scala> reduced
res32: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[15] at reduceByKey at <console>:34

scala> reduced.take(2)
res33: Array[(String, Int)] = Array((!=,3), (Master.,1))

scala> reduced.first()
res34: (String, Int) = (!=,3)

scala> var idfArr = reduced.map((a,b) => (Math.log(totalPosts) - Math.log(b))/Math.log(Math.E))
<console>:38: error: wrong number of parameters; expected = 1
         var idfArr = reduced.map((a,b) => (Math.log(totalPosts) - Math.log(b))/Math.log(Math.E))
                                        ^

scala> var idfArr = reduced.take(5).map(println)
(!=,3)
(Master.,1)
(someone,3)
(,fileFormatID,1)
(recursion,,1)
idfArr: Array[Unit] = Array((), (), (), (), ())

scala> var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._2))/Math.log(Math.E))
idfArr: Array[Double] = Array(4.189654742026425, 5.288267030694535, 4.189654742026425, 5.288267030694535, 5.288267030694535)

scala> var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._1))/Math.log(Math.E))
<console>:38: error: type mismatch;
 found   : String
 required: Double
         var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._1))/Math.log(Math.E))
                                                                                                  ^

scala> var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._0))/Math.log(Math.E))
<console>:38: error: value _0 is not a member of (String, Int)
         var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._0))/Math.log(Math.E))
                                                                                                  ^

scala> var idfArr = reduced.take(5).map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._2))/Math.log(Math.E))
idfArr: Array[Double] = Array(4.189654742026425, 5.288267030694535, 4.189654742026425, 5.288267030694535, 5.288267030694535)

scala> var idfArr = reduced.map(eachTuple => (Math.log(totalPosts) - Math.log(eachTuple._2))/Math.log(Math.E))
idfArr: org.apache.spark.rdd.RDD[Double] = MapPartitionsRDD[16] at map at <console>:38

scala> idfArr.take(10).foreach(println)
4.189654742026425
5.288267030694535
4.189654742026425
5.288267030694535
5.288267030694535
5.288267030694535
5.288267030694535
5.288267030694535
4.59511985013459
3.6788291182604347

//
// NEW LOG - 30/12/2017
//
scala> :paste
// Entering paste mode (ctrl-D to finish)

    // Location of Sequence Files
    val sample_index_loc = "spark-search-engine/sample_index"
    // val main_index_loc = "spark-search-engine/index"
    val index_loc = sample_index_loc
    // sc.saveAsObjectFile(index_loc) // Save RDDs as Spark Objects (Sequence Files)
    // sc.objectFile(index_loc + "/") // Load Spark Objects (Sequence Files) as RDDs

    //
    // Cosine Similarity
    //

    // Load IDF, TF_IDF Sets
    val idf_set = sc.objectFile[(String, Double)](sample_index_loc + "/idf")
    val tf_idf_set_opt_list = sc.objectFile[(String, (Int, Double))](sample_index_loc + "/tf_idf")

    //Get TF-IDF for Query
    val query_string = "big head"
    val query = sc.parallelize(query_string.toLowerCase.split(" "))
    val query_size = sc.broadcast(query.count().toDouble)
    val query_tf = query.map(query_term => (query_term, 1.0/query_size.value)).reduceByKey((a,b) => (a+b))
    val query_tf_idf = query_tf.join(idf_set).map(word => (word._1, word._2._1 * word._2._2))

    // Euclidean distance for Queries
    val query_ecd_distance = sc.broadcast(Math.sqrt(query_tf_idf.map(eachQuery => Math.pow(eachQuery._2, 2.0)).reduce(_ + _)))

    //Get Post TF-IDF
    //val posts_idf = query_idf
    val posts_filter_tf_idf_set = query_tf_idf.join(tf_idf_set_opt_list)

    // Sample
    // (code,(0.8278593324990533,(199,0.007701017046502822)))
    // (java,(1.6069316415223283,(89,0.08240675084729888)))
    // Dot Product = (query_tf_idf_1 * post_tf_idf) + (query_tf_idf_2 * post_tf_idf) ... + (query_tf_idf_n * post_tf_idf)
    // Euclidean distance = Math.sqrt(Math.pow(query_tf_idf_1,2) + Math.pow(query_tf_idf_2,2) ... + + Math.pow(query_tf_idf_n,2))
    // Cosine Similarity = Dot Product / Cosine Similarity
    // Formula = (query_tf_idf * post_tf_idf) / ( Math.sqrt(pow(query_tf_idf,2.0)) + Math.sqrt(pow(post_tf_idf,2.0)) )
    val posts_filter_cos = posts_filter_tf_idf_set.map(each_tf_idf_term_doc => (each_tf_idf_term_doc._2._2._1, ((each_tf_idf_term_doc._2._1 * each_tf_idf_term_doc._2._2._2), Math.pow(each_tf_idf_term_doc._2._2._2,2.0)) )).reduceByKey((a,b) => ((a._1+b._1), (a._2+b._2))).map(doc => (doc._1,(doc._2._1/query_ecd_distance.value * Math.sqrt(doc._2._2))))

    // OLD:
    // val posts_filter_cos = posts_filter_tf_idf_set.map(each_tf_idf_term_doc => (each_tf_idf_term_doc._2._2._1, ((each_tf_idf_term_doc._2._1 * each_tf_idf_term_doc._2._2._2), Math.pow(each_tf_idf_term_doc._2._1,2.0), Math.pow(each_tf_idf_term_doc._2._2._2,2.0)) )).reduceByKey((a,b) => ((a._1+b._1), (a._2+b._2), (a._3+b._3))).map(doc => (doc._1,(doc._2._1/(Math.sqrt(doc._2._2) * Math.sqrt(doc._2._3)))))

    val posts_filter_cos_sort = posts_filter_cos.map(row => (row._2, row)).sortByKey(false).map(row => (row._2))
    posts_filter_cos.foreach(println)

// Exiting paste mode, now interpreting.

(253,0.015923033241532443)
(371,0.0012959190001708388)
(308,9.49821901631347E-4)
(197,7.738028740689513E-4)
(129,5.0679671142791E-4)
sample_index_loc: String = spark-search-engine/sample_index
index_loc: String = spark-search-engine/sample_index
idf_set: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[1] at objectFile at <console>:39
tf_idf_set_opt_list: org.apache.spark.rdd.RDD[(String, (Int, Double))] = MapPartitionsRDD[3] at objectFile at <console>:40
query_string: String = big head
query: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[4] at parallelize at <console>:44
query_size: org.apache.spark.broadcast.Broadcast[Double] = Broadcast(3)
query_tf: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[6] at reduceByKey at <console>:46
query_tf_idf: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[10] at map at <console>:47
query_ecd_distance: org.apache.spark.broadcast.Broa...

scala> :paste
// Entering paste mode (ctrl-D to finish)

    val posts_filter_cos = posts_filter_tf_idf_set.map(each_tf_idf_term_doc => (each_tf_idf_term_doc._2._2._1, ((each_tf_idf_term_doc._2._1 * each_tf_idf_term_doc._2._2._2), Math.pow(each_tf_idf_term_doc._2._2._2,2.0)) )).reduceByKey((a,b) => ((a._1+b._1), (a._2+b._2))).map(doc => (doc._1,(doc._2._1/(query_ecd_distance.value * Math.sqrt(doc._2._2)))))


// Exiting paste mode, now interpreting.

posts_filter_cos: org.apache.spark.rdd.RDD[(Int, Double)] = MapPartitionsRDD[32] at map at <console>:47

scala> posts_filter_cos.foreach(println)
(253,0.7331494108168898)
(371,0.6800676006242671)
(308,0.680067600624267)
(197,0.680067600624267)
(129,0.7331494108168897)
